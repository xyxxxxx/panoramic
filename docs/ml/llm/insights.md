# 洞见

* [2312] [2312.11865](https://arxiv.org/abs/2312.11865) 尝试让 LLM 在文本环境中游玩《星际争霸 2》。应用 Chain of Summarization 方法（单帧总结提取游戏内信息，多帧总结分析局面并做出决策），LLM 的长期战略规划和战略决策能力显著提升。研究还发现 LLM 本身对于游戏内的知识（包括战术、流程等）就有一定程度的掌握，这有助于其进行分析决策。未来 LLM 与强化学习算法相结合，将有希望产生宏观决策与微观操作皆无懈可击的智能体。

* [2312] [2312.14302](https://arxiv.org/abs/2312.14302) 的研究指出 OpenAI 最近发布的 3 个 GPT-4 均存在着 vulnerability（还得考虑到 GPT-4 是当下能力最强的模型，并且 OpenAI 在安全性方面有着巨大的持续的投入）。在微调方面，几十上百个有害示例就足以让 GPT-3.5/4 产生有害行为，例如提供虚假信息，泄露隐私信息，生成有害代码等；有害示例可以很有针对性地只影响某一个点；即使是无害示例，也会削弱 GPT-3.5/4 的安全护栏。在应用方面，对于函数调用，模型很容易泄露函数调用模式，执行任意的未经检查的函数调用，以及被用于攻击应用本身；对于 RAG，可以通过在知识库的文档中插入提示词，使模型遵循新的提示词。

* [2312] Poe 上的 Gemini Pro 自曝自己是文心一言模型，令人十分怀疑其训练数据中包含了文心一言模型生成的内容。3 月就曾有[报道](https://www.theinformation.com/articles/alphabets-google-and-deepmind-pause-grudges-join-forces-to-chase-openai)称谷歌使用来自 ChatGPT 的数据训练 Bard，尽管[谷歌对此予以否认](https://www.theverge.com/2023/3/29/23662621/google-bard-chatgpt-sharegpt-training-denies)。无独有偶，字节跳动也刚被曝出秘密使用 ChatGPT 生成的文本训练 LLM。

    根据 [2305.17493](https://arxiv.org/abs/2305.17493v2)，这些使用生成数据训练的 LLM 会存在不可逆的缺陷。即便如此也要这么做的原因很可能是无法获取足够多高质量的文本数据。互联网上的公开数据正在被快速污染，从中提取高质量数据变得越来越困难。与此同时，内容的所有者也开始有意识地保护这些人工智能时代的石油。

    Epoch AI 的[一项研究](https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset)预测高质量的文本数据将在 2026 年之前枯竭，这可能延缓 LLM 的开发进展。UCB 教授 Stuart Russell 称训练 ChatGPT 等 LLM 可能很快耗尽全宇宙的文本。为了获取更多的高质量数据，OpenAI 急切地想要与世界上的大型出版商和创作者合作，一周前刚刚达成与 Springer 的合作伙伴关系。

    对于高质量的专有数据的访问权可能会成为 OpenAI 新的护城河，并且拉开闭源与开源模型之间的差距。获取更多的人类产生的高质量语料依然是目前应对“数据荒”的最佳方案，其他试图构建高质量数据的方法都存在一些问题。

* [2312] Karpathy 在一篇[推文](https://twitter.com/karpathy/status/1733299213503787018)中讨论了 LLM 的幻觉问题，建议阅读。

    “LLMs are dream machines” 这一观点具有启发性，我们可以把 LLM 类比为做梦的人脑：
    
    1. 它们都不断生成新的内容，并且自己无法控制生成的内容（但人做清醒梦时拥有自我意识，可以控制生成的内容；我们假定 LLM 没有自我意识）；
    1. 生成的内容从现实体验（训练文本）出发，可以符合，也可以脱离；尽管如此，做梦者不会发现自己在做梦；
    1. 梦和 LLM 都具有创造力，给人以灵感；
    1. 梦是多模态的，LLM 仅生成文本（或其拥有的模态）；
    1. 人的焦虑和欲望驱动梦的主题和内容（弗洛伊德的观点），而我们用 prompt 引导 LLM 生成的内容；
    1. 对于梦的功能存在许多假说，但生成文本对于 LLM 并不存在意义（只是一个做梦的机器）；
    1. 梦的机制在科学上尚未被完全理解，LLM 也是。

    既然 LLM 类似于做梦的人脑，那么我们可以直接用它去完成那些需要创造力的任务（或许人脑的基本用法也是做梦）。而如果我们想要用它去完成更加符合实际的任务，则需要唤醒它，下面是一些可能的方法：

    1. 让 LLM 使用各种工具操作环境，再通过各种感官接收来自环境的反馈（读万卷书，行万里路）；
    1. 调动 LLM 的思维。

* [2312] 继 6 月发布 phi-1，9 月发布 phi-1.5，微软再次[发布](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)了 SLM phi-2（2.7B）。依然是延续“textbooks are all you need”的思路，利用教科书级别的合成数据集训练 SLM 使其匹敌 LLM。在评估中，phi-2 的平均性能超越了 Mistral-7B。

    既然模型规模的扩张已经接近极限，如何高效利用模型参数自然成为接下来的一个研究重点。

    我们编写教材以促进人类学生学习，以同样的思路，我们合成“教科书级别”的训练数据以促进模型学习（相应的知识和能力）。

* [2305] 由于训练数据短缺，一些研究开始尝试使用模型生成的内容作为另一个模型的训练数据，但这一方法的正面和负面效果仍在探索和争论中。[2304.08466](https://arxiv.org/abs/2304.08466) 的实验显示 Imagen 生成的图片可以增强 ImageNet 训练集，显著提高训练在其上的模型的分类正确率；[2305.17493](https://arxiv.org/abs/2305.17493v2) 则发现在 LLM 的训练中使用另一个 LLM 生成的内容会导致最终 LLM 存在不可逆的缺陷。

* [2304] 涌现能力 论文 [2304.15004]()

